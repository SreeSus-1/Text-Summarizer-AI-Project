# üß† LLaMA Text Summarizer (Local GenAI Project)

An end-to-end Generative AI application that summarizes long text using a locally hosted LLaMA2 model via Ollama, with a FastAPI backend and Streamlit frontend.

## üöÄ Project Overview
This project demonstrates a full-stack GenAI architecture where users can input large text and receive concise summaries in real time. The application integrates a local LLM (LLaMA2) using Ollama, ensuring privacy, low latency, and no dependency on paid external APIs.

## üèóÔ∏è Architecture
User Input (Streamlit UI)  
‚Üí FastAPI Backend (REST API)  
‚Üí Ollama (LLaMA2 Local LLM)  
‚Üí Generated Summary (Returned to UI)

## ‚ú® Features
- Local LLM inference using LLaMA2 (via Ollama)
- Interactive Streamlit UI for real-time summarization
- FastAPI REST API backend
- End-to-end GenAI pipeline
- Robust error handling between frontend and backend
- Privacy-focused (no external API calls)

## üõ†Ô∏è Tech Stack
- Python
- FastAPI
- Streamlit
- Ollama
- LLaMA2
- Requests
- REST APIs


## ‚öôÔ∏è Installation & Setup

### 1Ô∏è‚É£ Clone the Repository
```bash
git clone https://github.com/SreeSus-1/Text-Summarizer-AI-Project.git
cd Text-Summarizer-AI-Project


2Ô∏è‚É£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate   # Windows


3Ô∏è‚É£ Install Dependencies
pip install -r requirements.txt

4Ô∏è‚É£ Install and Run Ollama (Local LLM)
Download: https://ollama.com

Then run:

ollama serve
ollama pull llama2

5Ô∏è‚É£ Start Backend (FastAPI)
uvicorn backend.main:app --reload --port 8000

6Ô∏è‚É£ Start Frontend (Streamlit)
streamlit run frontend/app.py

üåê API Endpoint
Request:

{
  "text": "Your long text here"
}
Response:

{
  "summary": "Generated summary text"
}
