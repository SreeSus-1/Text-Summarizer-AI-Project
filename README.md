# ğŸ§  LLaMA Text Summarizer (Local GenAI Project)

An end-to-end Generative AI application that summarizes long text using a locally hosted LLaMA2 model via Ollama, with a FastAPI backend and Streamlit frontend.

## ğŸš€ Project Overview
This project demonstrates a full-stack GenAI architecture where users can input large text and receive concise summaries in real time. The application integrates a local LLM (LLaMA2) using Ollama, ensuring privacy, low latency, and no dependency on paid external APIs.

## ğŸ—ï¸ Architecture
User Input (Streamlit UI)  
â†’ FastAPI Backend (REST API)  
â†’ Ollama (LLaMA2 Local LLM)  
â†’ Generated Summary (Returned to UI)

## âœ¨ Features
- Local LLM inference using LLaMA2 (via Ollama)
- Interactive Streamlit UI for real-time summarization
- FastAPI REST API backend
- End-to-end GenAI pipeline
- Robust error handling between frontend and backend
- Privacy-focused (no external API calls)

## ğŸ› ï¸ Tech Stack
- Python
- FastAPI
- Streamlit
- Ollama
- LLaMA2
- Requests
- REST APIs

## ğŸ“‚ Project Structure
Text-Summarizer-AI-Project/
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ app.py # Streamlit UI
â”‚
â”œâ”€â”€ backend/
â”‚ â””â”€â”€ main.py # FastAPI API with Ollama integration
â”‚
â”œâ”€â”€ venv/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/SreeSus-1/Text-Summarizer-AI-Project.git
cd Text-Summarizer-AI-Project


2ï¸âƒ£ Create Virtual Environment
python -m venv venv
venv\Scripts\activate   # Windows


3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Install and Run Ollama (Local LLM)
Download: https://ollama.com

Then run:

ollama serve
ollama pull llama2

5ï¸âƒ£ Start Backend (FastAPI)
uvicorn backend.main:app --reload --port 8000

6ï¸âƒ£ Start Frontend (Streamlit)
streamlit run frontend/app.py

ğŸŒ API Endpoint
Request:

{
  "text": "Your long text here"
}
Response:

{
  "summary": "Generated summary text"
}
